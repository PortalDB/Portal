\section{Demonstration Details}
\label{sec:demo}

\subsection{Demo Setup}
\label{sec:setup}

We will present an end-to-end implementation of \ql and demonstrate
some of the benefits it provides.  The demonstrate exhibits three main
components:

{\bf Interactive Shell:} We will have a console where users can type
in \ql commands and queries to explore the language features and its
expressiveness.  

{\bf Data Set:} We will have a local or remote
cluster with two small- to medium-sized evolving graph data sets that
allow interactive-speed data exploration.  DBLP~\cite{dblp} contains
co-authorship information from 1936 through 2015, with over 1.5
million author nodes and over 6 million undirected co-authorship
edges.  nGrams~\cite{nGrams} contains word co-occurrence information
from 1520 through 2008, with over 1.5 million word nodes and over 65
million undirected co-occurrence edges.

{\bf Web Query Composer:} Users will also be able to compose their
queries in the graphical composer and then execute them on the
cluster.

\subsection{Story Line}
\label{sec:story}

We demonstrate the functionality of \ql by executing queries provided
by us and by members of the audience.  Queries on very large evolving
graphs do not result in interactive time, but the DBLP data set we
will primarily use allows for exploratory querying can be processed
within several seconds on a small cluster.  The total run-time is
dominated by the data set loading time from HDFS, except in cases of
using snapshot analytics over long time periods. Some queries on the
nGrams data set also can be done in interactive time.  We will now
discuss a use case for each of the data sets.

{\bf Use Case 1:} DBLP data set can provide interesting insights into
Computer Science research community at large.  Which authors have the
most consistent publication record, i.e. publish every year over a
long time period? This question can be answered using temporal
aggregation.  How stable are the co-authorship relationships over
time? This question can also be answered using temporal aggregation.
Who are the most authoritative sources, as judged by the co-authorship
relationships, and does that conincide with our own judgements?  This
question can be answered using pagerank analytic over an aggregation
of various durations since the by-year data is very sparce.  Members
of the audience may come up with other questions.  We wil be recording
all the questions to see what language features, if any, are currently
missing from \ql but desirable to the community.

One of the most complicated queries that leads to meaningful results
is to answer the following question: find 10 researchers whose
publication careers exhibit the highest rising popularity over the
last 50 years of the 20th century.  We can use the following SQL-\ql
query:

\begin{small}
\begin{verbatim}
   Select vid, pr
   From (TSelect Any V[vid, trend(prank) as pr]
                 Any E
         From (TSelect All V[vid, pagerank() as prank]; 
                       All E
               From dblp
               TWhere Start >= 1950-01-01 And End < 2000-01-01
               TGroup by 5 years)
         TGroup by size).toVerticesFlat()
   Order by pr
   Limit 10
\end{verbatim}
\end{small}

{\bf Use Case 2:} Word co-occurrence data in the nGrams data set
provides interesting history of the evolution of the English language
over 400 years.  What is the rate of change?  By experimenting with
different aggregation sizes, we can see how stable two-word phrases
are long-term.  What 30 words are the most popular in each century, as
judged by the number of phrases that they appear in?  Users will be
able to explore the data set and come up with their own questions and
the queries to answer those questions.  They will be able to do so
both through the \qlui and the interactive shell.

