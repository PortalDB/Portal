For each slave:

1. Create instance.
  Go to https://cloud.cci.drexel.edu/dashboard
  Click on Instances.
  Click Launch Instance.
  Follow on-screen instructions. Pick unique name, like Slave17 if there are so far 16 slaves already created. Select the desired flavor - all slaves right now are 02c-08m-20d, which means 2 cores, 8G of RAM, 20G hard drive volume. From the Boot Source drop-down, pick Boot from image. Select Ubuntu Server 14.04 (2.2 GB) Image name. Add the local network dbgroup. Select root_waltz ssh key. If the key is not there, retrieve the public key from waltz - or ask Vera or Gaylord. When all configured, click Launch.
  Click on Volumes.
  Click Create Volume.
  Follow on-screen instructions. Pick corresponding name, like Slave17Hadoop. Set size. Current HDFS volumes are 200G each - this can grow in the future. Pick nova availability zone. Leave the other options with default values. Click Create Volume.
  Once the volume is in the list, click Edit Attachments for the volume through the drop-down on the right. Select the newly created instance. Click Attach Volume.

2. Basic configuration setup.
  Log in to waltz. Edit /etc/hosts file and add the new slave. 
  Copy the root key to the slave: 
    sudo scp updateRootAuthorizedKeys ubuntu@slave16:/home/ubuntu/
    sudo ssh ubuntu@slave16 "sudo bash updateRootAuthorizedKeys"
        (This will generate a "unable to resolve hostname" error which you may ignore, we'll fix it in the following actions.)
  Copy the /etc/hosts to all the cluster machines: (todo: simplify using ansible)
    sudo scp /etc/hosts root@master:/etc/hosts
    sudo scp /etc/hosts root@slave01:/etc/hosts
    sudo scp /etc/hosts root@slave02:/etc/hosts
    .... for each slave
    sudo scp /etc/hosts root@slave16:/etc/hosts

3. Upgrade the kernel 
  sudo ssh root@<slaveip>
  Note that until step 3 is completed, the slave may occasionally and randomly die with a kernel panic. If this happens, soft reboot the instance through the cloud web interface and proceed.
  aptitude update
  aptitude full-upgrade
  set the correct time (US Eastern): sudo dpkg-reconfigure tzdata
  reboot

  sudo ssh root@<slaveip>
  Label the attached hdfs volume with a slave#hadoop label: mkfs -L slave16hadoop /dev/vdb - verify first that it is /dev/vdb which is not guaranteed.
  Edit /etc/fstab to automount /opt on the label created on the previous line. For example, if the label added above is slave16hadoop, then run: 
      echo "LABEL=slave16hadoop /opt ext2 defaults 0 2" | tee -a /etc/fstab
  Disable ipv6:
      echo "#disable ipv6" | tee -a /etc/sysctl.conf
      echo "net.ipv6.conf.all.disable_ipv6 = 1" | tee -a /etc/sysctl.conf
      echo "net.ipv6.conf.default.disable_ipv6 = 1" | tee -a /etc/sysctl.conf
      echo "net.ipv6.conf.lo.disable_ipv6 = 1" | tee -a /etc/sysctl.conf
  Restart sysctl: 
      sysctl -p
  Test that ipv6 is off: (If value is 1, it's good. If that doesn't work, reboot and recheck.)
      cat /proc/sys/net/ipv6/conf/all/disable_ipv6  

4. Setup Hadoop group and user (as root)
  addgroup hadoop
  adduser --ingroup hadoop hduser     (dbgroup is the hduser password on all current slaves for ease of maintenance)
  adduser hduser sudo
  su - hduser
  mkdir .ssh
  chmod 700 .ssh
  exit (back to root)
  exit (back to waltz)

5. SSH setup
  Copy master's ssh keys and configuration: (replace slave16 with ip)
       sudo scp -3 root@master:/home/hduser/.ssh/id_rsa.pub root@slave16:/home/hduser/.ssh/id_rsa.pub
       sudo scp -3 root@master:/home/hduser/.ssh/id_rsa root@slave16:/home/hduser/.ssh/id_rsa
       sudo scp -3 root@master:/home/hduser/.ssh/authorized_keys root@slave16:/home/hduser/.ssh/authorized_keys
       sudo ssh root@<slaveip>
       chown hduser ~hduser/.ssh/id_rsa*
       chown hduser ~hduser/.ssh/authorized_keys
       chgrp hadoop ~hduser/.ssh/id*
       chgrp hadoop ~hduser/.ssh/authorized_keys 
       su - hduser

  Test by logging in from this slave to each machine as hduser: ssh master, etc. Also test ssh <slave> from master and previous slaves.

6. Install/setup Java 7
  If you are on waltz:
  sudo ssh root@<slaveip>
  add-apt-repository ppa:webupd8team/java
  aptitude update
  aptitude install oracle-java7-installer

  su - hduser
  export JAVA_HOME=/usr/lib/jvm/java-7-oracle/
  echo "" | tee -a $HOME/.bashrc
  echo "export JAVA_HOME=/usr/lib/jvm/java-7-oracle/" | tee -a $HOME/.bashrc
  check that java is good: java -version
  exit (to root)

7. Install/Configure Hadoop
  cd /opt/
  wget http://supergsego.com/apache/hadoop/core/stable/hadoop-2.6.0.tar.gz
  tar xzf hadoop-2.6.0.tar.gz
  mv hadoop-2.6.0 hadoop
  chown -R hduser:hadoop hadoop
  rm hadoop-2.6.0.tar.gz
  su - hduser
  as hduser:
  echo "export HADOOP_HOME=/opt/hadoop" | tee -a $HOME/.bashrc
  echo "export HADOOP_PREFIX=$HADOOP_HOME" | tee -a $HOME/.bashrc
  echo "" | tee -a $HOME/.bashrc
  echo "unalias fs &> /dev/null" | tee -a $HOME/.bashrc
  echo "alias fs='hadoop fs'" | tee -a $HOME/.bashrc
  echo "unalias hls &> /dev/null" | tee -a $HOME/.bashrc
  echo "alias hls='fs -ls'" | tee -a $HOME/.bashrc
  export HADOOP_HOME=/opt/hadoop
  echo "export PATH=$PATH:$HADOOP_HOME/bin" | tee -a $HOME/.bashrc
  mkdir /opt/hadoop/tmp
  let's check it all worked:
    source ~/.bashrc
    which hadoop
    hadoop version
    hls
  Update configuration files: (slave06 has the right configuration, don't change the ip here)
    scp hduser@slave06:/opt/hadoop/etc/hadoop/core-site.xml /opt/hadoop/etc/hadoop/core-site.xml 
    scp hduser@slave06:/opt/hadoop/etc/hadoop/hadoop-env.sh /opt/hadoop/etc/hadoop/hadoop-env.sh
    scp hduser@slave06:/opt/hadoop/etc/hadoop/hdfs-site.xml /opt/hadoop/etc/hadoop/hdfs-site.xml 
    scp hduser@slave06:/opt/hadoop/etc/hadoop/mapred-site.xml /opt/hadoop/etc/hadoop/mapred-site.xml
    scp hduser@slave06:/opt/hadoop/etc/hadoop/yarn-site.xml /opt/hadoop/etc/hadoop/yarn-site.xml 
  For this command, replace slave16 with the name of the slave
  echo "slave16" | tee /opt/hadoop/etc/hadoop/slaves

  Test the configuration on the slave: hadoop fs -ls hdfs://master:9000/data
  Add new slave to master configuration:
    ssh hduser@master 'echo "slave16" | tee -a /opt/hadoop/etc/hadoop/slaves'

  export HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop/
  Start the new slave hdfs data node on slave: $HADOOP_HOME/sbin/hadoop-daemon.sh --config "$HADOOP_CONF_DIR" --script "$bin/hdfs" start datanode
  Test: 
    hdfs dfsadmin -report  if the new slave shows up in the list with the expected capacity (just under 200GB), it's working.
  Note: to rebalance the disk utilization between data notes, run: hdfs balancer (it will take a long time).
  exit (to root)

8. Install/setup Mesos
  apt-key adv --keyserver keyserver.ubuntu.com --recv E56151BF
  DISTRO=$(lsb_release -is | tr '[:upper:]' '[:lower:]')
  CODENAME=$(lsb_release -cs)
  echo "deb http://repos.mesosphere.io/${DISTRO} ${CODENAME} main" | sudo tee /etc/apt/sources.list.d/mesosphere.list
  apt-get -y update
  apt-get -y install mesos
  service zookeeper stop
  sh -c "echo manual > /etc/init/zookeeper.override"
  echo "zk://master:2181/mesos" | sudo tee /etc/mesos/zk
  service mesos-master stop
  sh -c "echo manual > /etc/init/mesos-master.override"
  echo "HADOOP_HOME=/opt/hadoop" | sudo tee -a /etc/default/mesos-slave
  touch /etc/mesos-slave/?no-switch_user
     Yes, you want the question mark in the name of the file. It's what mesos expects.
  sh -c "echo /opt/hadoop > /etc/mesos-slave/hadoop_home"
  echo "1days" | tee /etc/mesos-slave/gc_delay
  service mesos-slave restart
  Test if it's working:
    MASTER=$(mesos-resolve `cat /etc/mesos/zk`)
    mesos-execute --master=$MASTER --name="cluster-test" --command="sleep 5"  
    If you see task finish, you are good.

9. Odds and ends
  Install svn: apt-get -y install subversion

DONE!
------------------
Sources:
http://dogdogfish.com/2014/04/22/hadoop-from-spare-change/
http://dogdogfish.com/2014/04/26/installing-hadoop-2-4-on-ubuntu-14-04/
http://www.bogotobogo.com/Hadoop/BigData_hadoop_Install_on_ubuntu_single_node_cluster.php
https://docs.mesosphere.com/getting-started/datacenter/install/
https://strat0sphere.wordpress.com/2014/10/30/spark-on-mesos-installation-guide/
