1. Get an account on waltz

2. edit your .bashrc file and add these lines:
#spark and hadoop
export HADOOP_HOME=/localhost/hadoop/hadoop-2.6.0
export SPARK_HOME=/localhost/spark/spark-1.3.1-bin-hadoop2.6/
export export LIBPROCESS_IP=192.168.126.12
export SPARK_LOCAL_IP=192.168.126.12
export HADOOP_PREFIX=$HADOOP_HOME
export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop

For temporaldata project, do just step 3, then skip to 6. For other projects, do all the steps.

3. get a copy of whatever git repo you are using with your code
git clone <path to git repo>

4. Create correct spark contest:
    //For mesos	cluster	execution use these 2 lines
    val conf = new SparkConf().setMaster("mesos://master:5050").setAppName("TemporalGraph Project")
        .set("spark.executor.uri", "hdfs://master:9000/spark/spark-1.3.1-bin-hadoop2.6.tgz")

    val sc = new SparkContext(conf)
    ProgramContext.setContext(sc)

5. Place assembly.sbt in src/project/target directory with this line in it:
addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.13.0")

6. Compile a fat jar (i.e. that has all the dependencies) by executing
sbt assembly

7. Submit the job to the cluster:
$SPARK_HOME/bin/spark-submit --class <name of class with main> --master mesos://master:5050 <name and location of fat jar> <arguments to your class>
For example:
$SPARK_HOME/bin/spark-submit --class edu.drexel.cs.dbgroup.graphxt.Driver --master mesos://master:5050 target/scala-2.10/tgraph-assembly-1.0.jar --data hdfs://master:9000/data/dblp --type SG --select 1970 1980 --count
