1. Get an account on waltz

2. edit your .bashrc file and add these lines:
#spark and hadoop
export HADOOP_HOME=/localhost/hadoop/hadoop-2.6.0
export SPARK_HOME=/localhost/spark/spark-1.4.0-bin-hadoop2.6/
export export LIBPROCESS_IP=192.168.126.12
export SPARK_LOCAL_IP=192.168.126.12
export HADOOP_PREFIX=$HADOOP_HOME
export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop

For temporaldata project, do just step 3, then skip to 6. For other projects, do all the steps.

3. get a copy of whatever git repo you are using with your code
git clone <path to git repo>

4. Create correct spark contest:
    //For mesos	cluster	execution use these 2 lines
    val conf = new SparkConf().setMaster("mesos://master:5050").setAppName("TemporalGraph Project")
        .set("spark.executor.uri", "hdfs://master:9000/spark/spark-1.3.1-bin-hadoop2.6.tgz")

    val sc = new SparkContext(conf)
    ProgramContext.setContext(sc)

5. Place assembly.sbt in src/project/target directory with this line in it:
addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.13.0")

6. Compile a fat jar (i.e. that has all the dependencies) by executing
sbt assembly
  Note: for temporaldata project, first compile the graphx extensions by following these steps:
    cd src/graphxext
    sbt package
    cp target/scala-2.10/graphx-extensions_2.10-1.0.jar ../main/lib
  This only should be done the first time and any time that the graphx extensions are modified, which is rare.

7. Submit the job to the cluster. For a single run, the spark-submit method can be used:
  $SPARK_HOME/bin/spark-submit --class <name of class with main> --master mesos://master:5050 <name and location of fat jar> <arguments to your class>
  For example:
  $SPARK_HOME/bin/spark-submit --class edu.drexel.cs.dbgroup.graphxt.Driver --master mesos://master:5050 target/scala-2.10/tgraph-assembly-1.0.jar --data hdfs://master:9000/data/dblp --type SG --select 1970 1980 --count
  -master local can be used for waltz-only, local executions without the rest of the cluster. This is useful for testing, for example.

  The temporaldata project, a Python driver pyDriver.py is provided that can more easily run one or multiple executions of spark, using the information provided in the config file. However, without code modification by defaul the driver saves all runtime information into the mysql database. This is desirable for experiments but undesirable for tests. If you use the pyDriver for tests, comment out the db saving code.
