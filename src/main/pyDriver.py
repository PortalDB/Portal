import sys;
sys.path.insert(0, './driverUtils')
import os;
import re;
import traceback;
import models, connect;
import subprocess;
import configparser;
import xml.etree.ElementTree as ET
from peewee import *;
from subprocess import Popen, PIPE;

#database = None;
#dbconnect = None;
strats = []

def collect_time(output):
    ftime = -1
    op_list = ["Aggregation", "Select", "PageRank", "Degree", "Materialize", "Trend", "Union", "Intersection", "Components"] #append to this list for new opearations
    time_dict = {}
    
    #collect final runtime
    r = re.compile('Final Runtime: (.*?)ms\n')
    m = r.search(output)

    if m:
        ftime = int(m.group(1))
        time_dict.update({0 : ftime})

    #collect times for each operation
    for op in op_list:
        searchStr = op + " Runtime: (.*?)\n";
        r = re.compile(searchStr)
        m = r.findall(output)

        if m:
            for val in m:
                res = val.split(" ")
                opTime = res[0].replace("ms", "") 
                seqNum = res[1].replace("(", "").replace(")", "") 
                time_dict.update({int(seqNum) : int(opTime)})

    return time_dict  

#helper fumction for collect_args
def get_agg_type(sem):
    if sem == "universal":
        return 0;
    return 1;   
    
#helper function for collect_args
def create_op(oT, a1, a2, pS, nP, rW):
    op = models.Operation(
        #op_id = _ _ _ (autogenerated in db)
        opType = oT,
        arg1 = a1,
        arg2 = a2,
        partitionS = pS,
        numParts = nP,
        runWidth = rW )
    return op

def collect_args(query, strat, run):
    line = query.split(" ");
    opDict = {} #dictionary of all operations in the query
    seqNum = 1
    opType = arg1 = arg2 = partS = numParts = runW = None
    addOp = False

    for i in range (0, len(line)):
        if line[i] == "group":
            opType = "Aggregate"
            arg1 = runW = int(line[i+2])
            arg2 = get_agg_type(line[i+5]) 
            partS = numParts = None

            addOp = True
    
        if line[i] == "where":
            opType = "Select"
            arg1 = int(line[i+1][6:].replace("-", ""))
            arg2 = int(line[i+3][4:].replace("-", ""))
            partS = strat
            runW = run
            numParts = 0

            #FIXME: need to put partition strategy here
            addOp = True

        if line[i] == "pagerank":
            opType = "PageRank"
            arg1 = int(line[i+4])
            arg2 = 1
            partS = runW = numParts = None

            addOp = True

        if line[i] == "degree":
            opType = "Degree"
            arg1 = arg2 = 1
            partS = runW = numParts = None

            addOp = True

        if line[i] == "components":
            opType = "Components"
            arg1 = arg2 = 1
            partS = runW = numParts = None

            addOp = True

        if line[i] == "materialize":
            opType = "Materialize"
            arg1 = arg2 = None # no args for this
            partS = runW = numParts = None            

            addOp = True
        
        if line[i] == "intersection":
            opType = "Intersection"
            arg1 = get_agg_type(line[i+2])
            arg2 = None
            partS = runW = numParts = None            

            addOp = True

        if line[i] == "union":
            opType = "Union"
            arg1 = get_agg_type(line[i+2])
            arg2 = None
            partS = runW = numParts = None            

            addOp = True

        #create new operation
        if addOp == True:
            newOp = create_op(opType, arg1, arg2, partS, numParts, runW)
            opDict.update({seqNum: newOp})
            seqNum += 1        
            addOp = False        

    #for a,b in opDict.iteritems():
    #    print a, "-", b.opID, b.opType, b.arg1, b.arg2, b.partitionS, b.numParts, b.runWidth
    return opDict

def genQueries(line, replaceIndex):
    #partitionS = ["CanonicalRandomVertexCut", "EdgePartition2D", "NaiveTemporal", "ConsecutiveTemporal", "HybridRandomTemporal", "Hybrid2DTemporal"]
    queries = []
    query = line[:]

    for strat in strats:
        if (strat != "None"):
            newStrat = "-p " + strat + " " +  str(part)
            
            query[replaceIndex] = newStrat
            newQuery = (" ".join(query)).replace(" -r", "")
            
            #prevent duplicates
            if not newQuery in queries: 
                queries.append(newQuery)
                
    return queries;

def run(configFile):
    global strats;

    parser = configparser.ConfigParser()
    parser.read(configFile)

    #load configurations from file
    mainc = parser['configs']['main']
    env = parser['configs']['env']
    mesosConf = parser['configs']['mesosConfig']
    localConf = parser['configs']['localConfig']
    ec2Conf = parser['configs']['ec2Config']
    standConf = parser['configs']['standaloneConfig']
    cConf = parser['configs']['clusterConfig']
    buildN = int(parser['configs']['buildNum'])
    itr = int(parser['configs']['iterations'])
    gType = parser['configs']['type']
    data = parser['configs']['data']
    dataset = parser['configs']['dataset']
    strats.extend(parser['configs']['strategy'].split(","))
    runw = parser['configs']['runwidth']

    #read queries
    c = parser['test_queries']['Queries'].split("\n")
    que = filter(None, c) #queries read from config file

    gtypeParam = " --type "
    dataParam = "--data "
    stratParam = " --strategy "
    widthParam = " --runWidth "
    sparkSubmit = "$SPARK_HOME/bin/spark-submit --class "
    envConf = localConf #set to local environment by default

    if env == "ec2":
        envConf = ec2Conf
    elif env == 'mesos':
        envConf = mesosConf    
    elif env == 'standalone':
        envConf = standConf
 
    #get git revision number and save build information
    p1 = Popen('cat ../../.git/refs/heads/master', stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
    gitRev = p1.communicate()[0];

    #read queries from file 
    for ln in que:
        line = ln.split(" ");
        qname = line[0]
        query = " ".join(line[1:-1]) + " " + line[-1].strip("\n") + " "

        #get cluster information when running on mesos cluster            
        if env == "mesos":
            #run sbt assembly
            Popen('sbt assembly', stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
            #get cluster config
            p2 = Popen('curl http://master:5050/slaves', stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
            out = p2.communicate()[0];
            numWorkers = out.count("\"active\":true")
            numCores = 2 #default num cores
            ram = 8 #default ram
            cConf = None

            r = re.compile('"cpus":(.*?),').search(out)
            if r:
                numCores = int(r.group(1))
                
            #set cluster config
            #FIXME: find ram of slaves
            cConf = str(numWorkers) + "s_" + str(numCores) + "c_" + str(ram) + "g"
    
        #get cluster information when running on standalone cluster    
        elif env == "standalone":
            #run sbt assembly
            Popen('sbt assembly', stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
            #get cluster config
            p2 = Popen('curl http://master:8081', stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
            out = p2.communicate()[0];
    
            out = out.replace(' ', '') 
            out = out.replace('\s', '') 
            out = out.replace('\t', '') 
            out = out.replace('\r', '') 
            out = out.replace('\n', '') 

            totalWorkers = numWorkers = totalCores = numCores = 0;
            ram = 16; #fixme: get actual ram 
            cConf = None

            #collect slave info
            r = re.compile('Workers:</strong>(.*?)</li>')
            m = r.search(out)

            if m:
                totalWorkers = int(m.group(1))            
                numDead = out.count('<td>DEAD</td>')    
                numWorkers = totalWorkers - numDead

            #collect cores info
            r = re.compile('Cores:</strong>(.*?)Total')
            m = r.search(out)

            if m:  
                totalCores = int(m.group(1))
                numCores = totalCores / totalWorkers
                
            #set cluster config
            #FIXME: find ram of slaves
            cConf = str(numWorkers) + "s_" + str(numCores) + "c_" + str(ram) + "g"  

        for strat in strats: 
            classArg = dataParam + data + gtypeParam + gType + stratParam + strat + widthParam + runw + " --query " + query
            querySaved = False;
            qRef = None;
            op_dict = id_dict = {}
            sparkCommand = sparkSubmit + mainc + " " + envConf + " " + classArg + " | tee -a log.out"           

            print "sparkCommand:", sparkCommand
            print "cluster Config:", cConf
            print "[STATUS]: running the spark-submit command against dataset and collect results..."
            
            for i in range (1, itr+1):
                p3 = Popen(sparkCommand, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True);
                pres = p3.communicate()
                output = pres[0]
                time_dict = collect_time(output);
                rTime = None
                print output                

                try:
                    rTime = time_dict[0] #get total runtime
                except KeyError:
                    print "ERROR: Query run did not return a final runtime. See result below:"
                    print pres[1]
                    print traceback.format_exc()
                    sys.exit(1)

                #only run this once for each query
                if querySaved == False:
                    qRef = dbconnect.persist_query() #persist to Query table
                    op_dict = collect_args(query, strat, runw);
                    id_dict = dbconnect.persist_ops(op_dict) #persist to Operation table
                    dbconnect.persist_query_ops(qRef, id_dict) #persist tp Query_Op_Map table            
                    querySaved = True        
 
                bRef = dbconnect.persist_buildRef(buildN, gitRev.strip("\n"))
                eRef = dbconnect.persist_exec(time_dict, qRef, gType, 0, cConf, rTime, i, bRef, dataset)
                dbconnect.persist_time_op(eRef, qRef, id_dict, time_dict) 
                print "[STATUS]: finished running iteration", i, "of current query..\n" 
    print "***  Done with executions." 

if __name__ == "__main__":
    database = models.BaseModel._meta.database
    dbconnect = connect.DBConnection(database)

    if(not len(sys.argv) > 1):
        print ("ERROR: you must pass in a temporal graph query config file to read from")
        exit();
    else:
       arg1 = sys.argv[1];
       run(arg1);
